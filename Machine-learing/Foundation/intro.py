Machine Learning is a way to make computers learn from data instead of being explicitly programmed.

In simple words:

Instead of telling the computer exact rules, we give it data + examples, and it figures out the rules by itself.



Machine Learning is a subset of Artificial Intelligence that enables systems to learn patterns from data and 
improve performance on a task without being explicitly programmed.



There are 3 main types (plus 1 advanced):

                    Supervised Learning

                    Unsupervised Learning

                    Reinforcement Learning

                    (Bonus) Semi-Supervised Learning


        1ï¸âƒ£ Supervised Learning (MOST IMPORTANT)
        Idea

        You train the model using labeled data.

        Two types inside Supervised Learning
        ğŸ”¹ A) Classification

        Output is a category

        Examples:

        Spam (yes / no)

        Fraud (true / false)

        ğŸ”¹ B) Regression

        Output is a number

        Examples:

        House price â†’ â‚¹45,00,000

        When to use Supervised Learning?

        âœ… When you have:

        Historical data

        Correct answers (labels)



    2ï¸âƒ£ Unsupervised Learning
            Idea

            No labels âŒ
            Model finds patterns on its own.

             Example

                You have customers, but no categories.

                Model groups them like:

                High spenders

                Medium spenders

                Low spenders

                You didnâ€™t tell it these groups â€” it discovered them.

            Common tasks
            ğŸ”¹ Clustering

                     Grouping similar data points.                    
            ğŸ”¹ Dimensionality Reduction

                    Reduce features, keep important info.




    3ï¸âƒ£ Reinforcement Learning (RL)
    Idea

    Learning by trial and error using rewards.



    4ï¸âƒ£ Semi-Supervised Learning (Bonus)
            Idea

            Mix of:

            Small labeled data

            Large unlabeled data

            Used when labeling is expensive.



What is a Feature?
Simple definition

A feature is an individual measurable property (input variable) used by a machine learning model to 
make a prediction.





Feature Engineering (SUPER IMPORTANT)
What is it?

Creating useful features from raw data


How many features should you have?
        Thereâ€™s no fixed number, but:
        Too few â†’ underfitting
        Too many â†’ noise, overfitting
        Rule of thumb:
        Quality > Quantity

Features are the signals the model uses to make decisions.



What is a Label?
Simple definition
A label is the correct answer (ground truth) that the model is trying to predict.



What is training?
Training is the process where a model learns patterns by comparing its predictions with 
labels and correcting itself.

What happens during training?
Model takes features (X)
Makes a prediction
Compares with labels (Y)
Calculates error (loss)
Updates internal parameters
Repeats many times ğŸ”


ğŸ”¹ Inference (Prediction)
What is inference?
Using a trained model to make predictions on new, unseen data.
What happens during inference?
New features come in
Model applies learned patterns
Outputs prediction
No learning happens here âŒ

Very important concept: Model Freeze
Once training is done:
Model weights are frozen
Inference must not modify the model
If it does â†’ data leakage + chaos ğŸš¨

One-line memory hook
Training teaches. Inference answers.



ğŸ”¹ Underfitting
What is underfitting?
Model is too simple and fails to learn the patterns in the data.

How to fix UNDERFITTING

âœ… Add better features
âœ… Use more powerful model
âœ… Train longer
âœ… Reduce regularization




ğŸ”¹ Overfitting
What is overfitting?
Model learns training data too well â€” including noise â€” and fails on new data.
How to fix OVERFITTING (very practical)

âœ… More data
âœ… Feature selection
âœ… Regularization
âœ… Early stopping
âœ… Simpler 

One-line memory hook
Underfitting = didnâ€™t learn enough
Overfitting = learned too much (including junk)





what are Bias and Variance?
ğŸ”¹ Bias
Error due to overly simple assumptions in the model.
Bias says:
â€œThe world is simple. Iâ€™ll ignore details.â€

ğŸ”¹ Variance
Error due to model being too sensitive to training data.
Variance says:
â€œEvery small detail matters. Iâ€™ll memorize everything.â€







Why do we split data at all?
Because MLâ€™s real job is:
Perform well on unseen data

The three splits
1ï¸âƒ£ Training Set
Purpose:
ğŸ‘‰ Teach the model.
2ï¸âƒ£ Validation Set
Purpose:
ğŸ‘‰ Tune decisions.
3ï¸âƒ£ Test Set
Purpose:
ğŸ‘‰ Final exam.



What is Data Leakage? ğŸš¨
Data leakage happens when information from validation/test leaks into training â€” directly or indirectly.


Common leakage examples (VERY IMPORTANT)
âŒ 1. Using future information
Example:
Feature: account_suspended_in_next_7_days
ğŸ’€ Model is literally seeing the answer.


One-line memory hook
Train to learn, validate to decide, test to trust.




ML data cleaning is about protecting the learning signal â€” not making data look neat.
In ML, messy data can be informative. Clean data can be dangerous.


What is Feature Engineering?
Feature engineering is the process of creating, transforming, and selecting features so 
that a model can learn meaningful patterns.


Feature engineering is teaching the model what to pay attention to.








Your domain-specific goldmine ğŸ”¥

For your prediction system, high-value features likely are:

Time-window aggregates

Behavior change rates

Ratios instead of raw counts

Early-life behavior (first X days)

Sudden spikes / anomalies

How to handle class imbalance (REAL fixes)

1ï¸âƒ£ Use correct metrics (FIRST STEP)

âŒ Accuracy
âœ… Precision, Recall, F1
âœ… Confusion Matrix
âœ… PR-AUC



2ï¸âƒ£ Change class weights (BEST first fix)

Tell model:

â€œClass 1 mistakes matter moreâ€

Example:

class_weight = {0:1, 1:10}

This alone often fixes â€œpredicts only 0â€.



3ï¸âƒ£ Resampling (careful)

Oversampling (minority)

SMOTE

Random oversampling

Pros:

Helps learning

Cons:

Overfitting risk

Undersampling (majority)

Remove some 0s

Pros:

Faster
Cons:

Data loss


Most ML problems fail not because of models, but because minority signals are ignored.















Scaling & Encoding

Big idea (remember this)

Models only understand numbers, but not all numbers are equal.

Scaling = make numbers comparable
Encoding = turn categories into numbers without lying

PART 1: Scaling
What is scaling?

Transforming numerical features so theyâ€™re on a similar range.

When scaling is REQUIRED
ğŸš¨ Scale for these models
These models care about distance / magnitude:
Logistic Regression
Linear Regression
SVM
KNN
Neural Networks
Why?
They compute distances or weighted sums

When scaling is NOT required
âŒ Donâ€™t worry for:
Decision Trees
Random Forest
XGBoost
They split by thresholds, not distances.
Scaling doesnâ€™t hurt, but doesnâ€™t help much.


PART 2: Encoding
Why encoding?
Models cannot understand text:
"mobile", "desktop"
We convert categories â†’ numbers carefully.





Scaling + Encoding order (IMPORTANT)
Correct pipeline:
Split data
 â†’ Encode categorical
 â†’ Scale numerical
 â†’ Train model

Never:
Scale before split
Encode using future info





